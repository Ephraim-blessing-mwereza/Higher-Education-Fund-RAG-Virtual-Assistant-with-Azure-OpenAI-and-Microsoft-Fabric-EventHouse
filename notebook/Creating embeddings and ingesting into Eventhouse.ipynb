{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ad43d75-6a42-4b60-9338-1bbc4dd49732",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Building a RAG  HEF Assistant with Fabric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d61d8e-d4ec-4815-ba89-f1fc28ff69b1",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "%pip install openai==1.12.0 azure-kusto-data langchain tenacity langchain-openai pypdf\n",
    "%pip install beautifulsoup4 langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29c4b81-d65a-4c51-b58c-593975d23edf",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "%pip install openai --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b8ee10-45c5-4f3d-9d8a-e929c863cd27",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "from IPython.display import display, HTML\n",
    "import os\n",
    "import textwrap\n",
    "import json \n",
    "import requests\n",
    "import pandas as pd\n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "from notebookutils import mssparkutils\n",
    "from azure.kusto.data import KustoClient, KustoConnectionStringBuilder\n",
    "from azure.kusto.data.exceptions import KustoServiceError\n",
    "from azure.kusto.data.helpers import dataframe_from_result_table\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter,RecursiveCharacterTextSplitter\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
    "from bs4 import SoupStrainer\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98e7982-0606-4a3d-8a06-268f5ac792ff",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "OPENAI_GPT4_DEPLOYMENT_NAME=\"gpt-4o-kenya-hack\"\n",
    "OPENAI_DEPLOYMENT_ENDPOINT=\"Your-OpenAI-Endpoint\" # Replace with your OpenAI endpoint\n",
    "OPENAI_API_KEY=\"Your-OpenAI-API-KEY\" # Replace with your OpenAI API key\n",
    "OPENAI_ADA_EMBEDDING_DEPLOYMENT_NAME = \"text-embedding-ada-002-kenya-hack\"\n",
    "\n",
    "\n",
    "KUSTO_URI = 'Your-Kusto-URI' # Replace with your kusto URI\n",
    "KUSTO_DATABASE = \"HEF_eventhouse\"\n",
    "KUSTO_TABLE = \"hefEmbeddings\"\n",
    "accessToken = mssparkutils.credentials.getToken(KUSTO_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fac1fe1-71ed-4fd8-82bf-33cf0d867170",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "##### Creating an Azure OpenAI client and defining a function to calculate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa49e866-7f4e-41f6-a561-b11833309559",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "client = AzureOpenAI(\n",
    "        azure_endpoint=OPENAI_DEPLOYMENT_ENDPOINT,\n",
    "        api_key=OPENAI_API_KEY,\n",
    "        api_version=\"2023-09-01-preview\"\n",
    "    )\n",
    "\n",
    "#we use the tenacity library to create delays and retries when calling openAI embeddings to avoid hitting throttling limits\n",
    "@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))\n",
    "def generate_embeddings(text): \n",
    "    # replace newlines, which can negatively affect performance.\n",
    "    txt = text.replace(\"\\n\", \" \")\n",
    "    return client.embeddings.create(input = [txt], model=OPENAI_ADA_EMBEDDING_DEPLOYMENT_NAME).data[0].embedding\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776e0e76-65af-4d1a-9568-48076c6ea709",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "##### Reading the pdf files, divide it into 1000 chars chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb964d3c-fa39-44f3-9e75-cb65d9dcc573",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# splitting into 1000 char long chunks with 30 char overlap\n",
    "# split [\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=30,\n",
    ")\n",
    "\n",
    "# List of PDF files (adjust filenames as per your lakehouse)\n",
    "pdf_files = [\n",
    "    {\"name\": \"UF-FAQs.pdf\", \"path\": \"/lakehouse/default/Files/UF-FAQs.pdf\"},\n",
    "    {\"name\": \"HEF-NFM-FAQs.pdf\", \"path\": \"/lakehouse/default/Files/HEF-NFM-FAQs.pdf\"},  \n",
    "    {\"name\": \"Helb-FAQS.pdf\", \"path\": \"/lakehouse/default/Files/Helb-FAQS.pdf\"}, \n",
    "    {\"name\": \"University-Funding-FAQs2.pdf\", \"path\": \"/lakehouse/default/Files/University-Funding-FAQs2.pdf\"},\n",
    "]\n",
    "\n",
    "# List of web URLs\n",
    "web_urls = [\n",
    "    \"https://www.helb.co.ke/faqs/students-faqs/\",\n",
    "    \"https://www.helb.co.ke/faqs/loanees-faqs/\",\n",
    "    \"https://www.helb.co.ke/faqs/employers-faqs/\",\n",
    "    \"https://www.helb.co.ke/faqs/institutions-faqs/\",\n",
    "    \"https://www.hef.co.ke/\",\n",
    "    \"https://www.hef.co.ke/faqs/\",\n",
    "    \"https://kuccps.net/frequently-asked-questions\",\n",
    "    \"https://www.universitiesfund.go.ke/blog/frequently-asked-questions/\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf90b1e-6a8d-48f9-8266-e0ffd802c70f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# Load PDFs\n",
    "all_pages = []\n",
    "for pdf in pdf_files:\n",
    "    try:\n",
    "        loader = PyPDFLoader(pdf[\"path\"])\n",
    "        pages = loader.load_and_split(text_splitter=splitter)\n",
    "        print(f\"Loaded {len(pages)} chunks from {pdf['name']}\")\n",
    "        all_pages.extend(pages)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {pdf['name']}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee0fe9f-b7cc-4fa8-a01e-201dc8799583",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# Load web content\n",
    "for url in web_urls:\n",
    "    try:\n",
    "        # Try WebBaseLoader with SSL verification disabled\n",
    "        loader = WebBaseLoader(url, verify_ssl=False)\n",
    "        pages = loader.load_and_split(text_splitter=splitter)\n",
    "        print(f\"Loaded {len(pages)} chunks from {url}\")\n",
    "        all_pages.extend(pages)\n",
    "    except Exception as e:\n",
    "        print(f\"WebBaseLoader failed for {url}: {e}\")\n",
    "        # Fallback: Use requests directly\n",
    "        try:\n",
    "            response = requests.get(url, verify=False)  # Bypass SSL verification\n",
    "            response.raise_for_status()  # Check for HTTP errors\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            text = soup.get_text(separator=\" \")  # Extract all text\n",
    "            \n",
    "            # Create a single Document object manually\n",
    "            from langchain.docstore.document import Document\n",
    "            doc = Document(page_content=text, metadata={\"source\": url})\n",
    "            pages = splitter.split_documents([doc])\n",
    "            print(f\"Fallback loaded {len(pages)} chunks from {url}\")\n",
    "            all_pages.extend(pages)\n",
    "        except Exception as fallback_e:\n",
    "            print(f\"Fallback failed for {url}: {fallback_e}\")\n",
    "\n",
    "# Total chunks\n",
    "print(\"Total number of chunks: \", len(all_pages))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfca7626-31e5-425f-8095-10b1ccefdf30",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "##### Saving the text chunks to a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1803a4d6-73af-4758-ae50-9c4f74574208",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# Save to DataFrame\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(columns=['document_name', 'content', 'embedding'])\n",
    "for page in all_pages:\n",
    "    # Use source (URL or file path) as document name\n",
    "    doc_name = page.metadata.get('source', 'Unknown PDF')\n",
    "    df.loc[len(df.index)] = [doc_name, page.page_content, \"\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06214cf1-66ae-44bc-adb0-6dd1549062d0",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "##### Calculating embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57db0e60-91a0-41c6-8e70-2cd5e0235f78",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# Generate embeddings (assuming generate_embeddings is defined)\n",
    "df[\"embedding\"] = df.content.apply(lambda x: generate_embeddings(x))\n",
    "print(df.head(2))\n",
    "\n",
    "# Optional: Save DataFrame to a file or database for later use\n",
    "# df.to_csv(\"/lakehouse/default/Files/combined_embeddings.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75087c6-7838-4e88-b5a4-f2403dc2c6f9",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "##### Writing the data to MS Fabric Eventhouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8086768",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "df_sp = spark.createDataFrame(df)\n",
    "\n",
    "df_sp.write.\\\n",
    "format(\"com.microsoft.kusto.spark.synapse.datasource\").\\\n",
    "option(\"kustoCluster\",KUSTO_URI).\\\n",
    "option(\"kustoDatabase\",KUSTO_DATABASE).\\\n",
    "option(\"kustoTable\", KUSTO_TABLE).\\\n",
    "option(\"accessToken\", accessToken ).\\\n",
    "mode(\"Append\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5be7384-5b76-4481-bc7c-1e6a504bf90b",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Vector search on Fabric Eventhouse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee864bae-b08d-4bdb-92ff-baa22a2c9ea7",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "##### A function to calling GPT4 for a Natural Language answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c07e41-f19f-4141-9add-3e0a1ae2b4fc",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def call_openAI(text):\n",
    "    response = client.chat.completions.create(\n",
    "        model=OPENAI_GPT4_DEPLOYMENT_NAME,\n",
    "        messages = text,\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de7a2ab-8d1b-49da-bdbb-e2b2c0a140a1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#####  A function  retrieving answers using embeddings with similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a1573c-b896-41ae-b9e9-cb04663ab1a4",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def get_answer_from_eventhouse(question, nr_of_answers=1):\n",
    "        searchedEmbedding = generate_embeddings(question)\n",
    "        kusto_query = KUSTO_TABLE + \" | extend similarity = series_cosine_similarity(dynamic(\"+str(searchedEmbedding)+\"), embedding) | top \" + str(nr_of_answers) + \" by similarity desc \"\n",
    "        kustoDf  = spark.read\\\n",
    "        .format(\"com.microsoft.kusto.spark.synapse.datasource\")\\\n",
    "        .option(\"kustoCluster\",KUSTO_URI)\\\n",
    "        .option(\"kustoDatabase\",KUSTO_DATABASE)\\\n",
    "        .option(\"accessToken\", accessToken)\\\n",
    "        .option(\"kustoQuery\", kusto_query).load()\n",
    "\n",
    "        return kustoDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03a79d5-c0fd-49e7-9871-d95b040bd395",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# Retrieves 2 answers from Eventhouse\n",
    "nr_of_answers = 2\n",
    "question = \"Can IGCSE graduates apply to KUCCPS for placement to universities and colleges?\"\n",
    "answers_df = get_answer_from_eventhouse(question, nr_of_answers)\n",
    "\n",
    "# Concatenates the answers\n",
    "answer = \"\"\n",
    "for row in answers_df.rdd.toLocalIterator():\n",
    "    answer = answer + \" \" + row['content']\n",
    "\n",
    "# Creates a prompt for GPT4 with the question and the 2 answers\n",
    "prompt = 'Question: {}'.format(question) + '\\n' + 'Information: {}'.format(answer)\n",
    "# prepare prompt\n",
    "messages = [{\"role\": \"system\", \"content\": \"You are a HELPFUL assistant answering users questions. Answer the question using the provided information and do not add anything else.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "result = call_openAI(messages)\n",
    "display(result)\n"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "20c0620b-e82e-4c71-a4ad-2dd2017ff944",
    "default_lakehouse_name": "Hef_Lakehouse",
    "default_lakehouse_workspace_id": "ec5a7aca-e27f-4992-80fd-e88ce676daca",
    "known_lakehouses": [
     {
      "id": "20c0620b-e82e-4c71-a4ad-2dd2017ff944"
     }
    ]
   }
  },
  "kernel_info": {
   "jupyter_kernel_name": "python3.11",
   "name": "jupyter"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "jupyter_python",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "orig_nbformat": 4,
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  },
  "widgets": {}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
